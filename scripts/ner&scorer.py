# -*- coding: utf-8 -*-
"""NER&Scorer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jnwn_EklvRh_OzzfDXxR8Uu2_pxQSVVq
"""

# Importing required libraries.

import json
import spacy
from spacy.tokens import Doc
from spacy.training import Example

# 1st Code: Using NER

#PART 1: SET-UP
with open("Vol1StoreOCRPerPage.json") as f: #more efficient than f = open(...); no need to close
   d = json.load(f) #store file contents in d (dictionary)


nlp = spacy.load("en_core_web_sm")


text1 = d["665"] #load page 665 of VCR Vol 1
test1 = nlp(text1)
pred_ents = [(ent.start_char, ent.end_char, ent.text, ent.label_) for ent in test1.ents if ent.label_ in ("GPE, LOC")] #predicted entities
print(pred_ents)
entities = []

import spacy
from spacy.training import Example
from spacy.scorer import Scorer

# Load your NER model
nlp = spacy.load("en_core_web_sm")  # or your own trained model

# Define gold examples (texts and correct entity spans)
examples_data = [
    {
        "text": "studied by investigators who can not come to Washington",
        "entities": [(45, 55, "GPE")]
    },
    {
        "text": "have found a lodgment within the United States",
        "entities": [(29, 46, "GPE")]
    }
]

examples_data[0]["text"][45:55] # Ensure positions are correct
examples_data[1]["text"][29:46]

# Convert to spaCy Example objects
examples = []
for example in examples_data:
    text = example["text"]
    ents = example["entities"]

    doc_pred = nlp(text)
    doc_gold = nlp.make_doc(text)
    spans = [doc_gold.char_span(start, end, label) for start, end, label in ents]
    spans = [span for span in spans if span is not None]
    doc_gold.ents = spans

    example = Example(predicted=doc_pred, reference=doc_gold)
    examples.append(example)

# Score
scorer = Scorer()
scores = scorer.score(examples)

print("Precision:", scores["ents_p"])
print("Recall:", scores["ents_r"])
print("F1:", scores["ents_f"])

# 2nd Code: Using NER to find all labels with costum functions

data=("text")

#PART 0: SET-UP
import spacy
import json
from spacy.tokens import Doc
from spacy.training import Example
from spacy.scorer import Scorer

nlp = spacy.load("en_core_web_sm")


with open("data2.json", "w") as f:
    json.dump(data, f, ensure_ascii='False')


chunk = nlp(data)

# Helpful Functions

def pos_finder(string, text):
    '''This function returns the position (start index, end index) of a specified string in an
    unprocessed text as a tuple.'''
    return (text.index(string), text.index(string) + len(string))

def ent_info_extract(text):
    '''This function returns a list of tuples; each tuple contains the start index of the entity in
    the specified processed text, the end index of the entity in the text, and the entity label.'''
    return [(ent.start_char, ent.end_char, ent.label_) for ent in text.ents]

def ent_info_to_edit(text):
    '''This function returns a list of tuples; each tuple contains the start index of the entity in
    the specified processed text, the end index of the entity in the text, the entity label, and the entity text.
    It serves as a more readable/editable list than that returned by ent_info_extract.'''
    return [(ent.start_char, ent.end_char,ent.text, ent.label_) for ent in text.ents]

ent_info_extract(chunk)

ent_info_to_edit(chunk)

# Using Scorer

with open("Vol1StoreOCRPerPage.json") as file:
  Vol1=json.load(file)

# To use Example, golden standard (entities) should be provided, which we manually extracted based on the chosen page.
train_data = [(Vol1['583'], {"entities": [(0, 6, 'DATE'), (8, 12, 'DATE'), (67, 73, 'ORG'), (112, 117, 'PERSON'), (135, 141, 'ORG'), (202, 203, 'CARDINAL'), (208, 216, 'PERSON'), (232, 233, 'CARDINAL'), (272, 273, 'CARDINAL'), (302, 310, 'GPE'), (326, 329, 'CARDINAL'), (331, 344, 'PERSON'), (399, 412, 'PERSON'), (364, 371, 'ORG'), (463, 481, 'LOC'), (495, 504, 'QUANTITY'), (551, 553, 'CARDINAL'), (642, 653, 'PERSON'), (689, 698, 'PERSON'), (731, 749, 'LOC'), (765, 773, 'GPE'), (887, 897, 'QUANTITY'), (935, 941, 'ORG'), (943, 946, 'CARDINAL'), (951, 959, 'GPE'), (973, 989, 'GPE'), (990, 999, 'QUANTITY'), (1004, 1018, 'GPE'), (1020, 1023, 'CARDINAL'), (1029, 1036, 'GPE'), (1043, 1046, 'CARDINAL'), (1052, 1064, 'GPE'), (1143, 1145, 'CARDINAL'), (1188, 1190, 'CARDINAL'), (1227, 1229, 'CARDINAL'), (1337, 1351, 'ORG'), (1378, 1381, 'CARDINAL'), (1440, 1443, 'CARDINAL'), (1587, 1592, 'ORG'), (1774, 1779, 'PERSON'), (1858, 1867, 'QUANTITY'), (1904, 1910, 'ORG'), (1916, 1918, 'CARDINAL'), (1919, 1928, 'PERSON'), (1979, 1988, 'DATE'), (2022, 2031, 'PERSON'), (2127, 2134, 'PERSON'), (2189, 2195, 'ORG'), (2314, 2319, 'LAW'), (2377, 2384, 'ORG'), (2420, 2426, 'ORG'), (2444, 2450, 'GPE'), (2539, 2547, 'ORG')]})]

examples = []
for text, annotations in train_data:
    doc = nlp.make_doc(text)
    example = Example.from_dict(doc, annotations)
    examples.append(example)

predicted_examples = []
for example in examples:
    # Run the model on the input text
    pred_doc = nlp(example.text)
    # Create a new Example comparing the prediction to the gold annotations
    predicted_example = Example(pred_doc, example.reference)
    predicted_examples.append(predicted_example)


# Use spaCy's scorer
scorer = Scorer()
scores = scorer.score(predicted_examples)

print(scores)

# 3rd code: Costum function that uses NER
import spacy
from spacy.tokens import Span
from spacy.training import Example
from spacy.scorer import Scorer

def evaluate_ner(text, predicted_ents, gold_ents):
    nlp = spacy.blank("en")
    doc = nlp.make_doc(text)

    doc_pred = doc.copy()
    doc_pred.ents = [Span(doc_pred, *doc.char_span(s, e).indices, label) for s, e, label in predicted_ents if doc.char_span(s, e)]

    doc_gold = doc.copy()
    doc_gold.ents = [Span(doc_gold, *doc.char_span(s, e).indices, label) for s, e, label in gold_ents if doc.char_span(s, e)]

    example = Example(doc_pred, doc_gold)
    return Scorer().score([example])

# Using the function
predicted_ents = ent_info_extract(chunk)
gold_entities = [(0, 6, 'DATE'), (8, 12, 'DATE'), (67, 73, 'ORG'), (112, 117, 'PERSON'), (135, 141, 'ORG'), (202, 203, 'CARDINAL'), (208, 216, 'PERSON'), (232, 233, 'CARDINAL'), (272, 273, 'CARDINAL'), (302, 310, 'GPE'), (326, 329, 'CARDINAL'), (331, 344, 'PERSON'), (399, 412, 'PERSON'), (364, 371, 'ORG'), (463, 481, 'LOC'), (495, 504, 'QUANTITY'), (551, 553, 'CARDINAL'), (642, 653, 'PERSON'), (689, 698, 'PERSON'), (731, 749, 'LOC'), (765, 773, 'GPE'), (887, 897, 'QUANTITY'), (935, 941, 'ORG'), (943, 946, 'CARDINAL'), (951, 959, 'GPE'), (973, 989, 'GPE'), (990, 999, 'QUANTITY'), (1004, 1018, 'GPE'), (1020, 1023, 'CARDINAL'), (1029, 1036, 'GPE'), (1043, 1046, 'CARDINAL'), (1052, 1064, 'GPE'), (1143, 1145, 'CARDINAL'), (1188, 1190, 'CARDINAL'), (1227, 1229, 'CARDINAL'), (1337, 1351, 'ORG'), (1378, 1381, 'CARDINAL'), (1440, 1443, 'CARDINAL'), (1587, 1592, 'ORG'), (1774, 1779, 'PERSON'), (1858, 1867, 'QUANTITY'), (1904, 1910, 'ORG'), (1916, 1918, 'CARDINAL'), (1919, 1928, 'PERSON'), (1979, 1988, 'DATE'), (2022, 2031, 'PERSON'), (2127, 2134, 'PERSON'), (2189, 2195, 'ORG'), (2314, 2319, 'LAW'), (2377, 2384, 'ORG'), (2420, 2426, 'ORG'), (2444, 2450, 'GPE'), (2539, 2547, 'ORG')]
scores = evaluate_ner(data, predicted_ents, gold_entities)  # your gold list
print(scores)