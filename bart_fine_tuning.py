# -*- coding: utf-8 -*-
"""BART Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Af-m1CfGqgc_bwcvwsfd1S1uf0yk1xPT
"""

pip install transformers datasets torch

from google.colab import drive
drive.mount('/content/drive')

# Training BART on text only

import os
import json
import math
from datetime import datetime
from transformers import (
    BartTokenizer,
    BartForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
)
from datasets import Dataset

# Use this code only if you have ready files for Dirty Texts.
# If you only have cleaned text, use the next code to simulate the dirty text for you.

# ---------- Config ----------
dirty_dir = "/content/drive/MyDrive/Dirty Texts"   # uncleaned OCR
clean_dir = "/content/drive/MyDrive/Texts"        # cleaned target
chunk_size = 10
output_base = "bart_recursive_chunks"



# ---------- Load Records ----------
records = []

dirty_files = sorted([
    f for f in os.listdir(dirty_dir) if f.endswith(".txt") and os.path.isfile(os.path.join(dirty_dir, f))
])
clean_files = sorted([
    f for f in os.listdir(clean_dir) if f.endswith(".txt") and os.path.isfile(os.path.join(clean_dir, f))
])

if len(dirty_files) != len(clean_files):
    raise ValueError(f"Mismatch in file count: {len(dirty_files)} dirty vs {len(clean_files)} clean")

for dirty_file, clean_file in zip(dirty_files, clean_files):
    with open(os.path.join(dirty_dir, dirty_file), "r", encoding="utf-8") as f_dirty:
        dirty_text = f_dirty.read().strip()
    with open(os.path.join(clean_dir, clean_file), "r", encoding="utf-8") as f_clean:
        clean_text = f_clean.read().strip()
    records.append({"ocr": dirty_text, "clean": clean_text})

print(f"Total records loaded: {len(records)}")

import os
import random
import re

# Define directories
clean_dir = "/content/drive/MyDrive/Texts"
sim_dirty_dir = "/content/drive/MyDrive/Simulated Dirty Texts"

# Create the simulated dirty directory if it doesn't exist
os.makedirs(sim_dirty_dir, exist_ok=True)

# List all cleaned text files
clean_files = sorted([
    f for f in os.listdir(clean_dir) if f.endswith(".txt") and os.path.isfile(os.path.join(clean_dir, f))
])

# Function to introduce simulated OCR errors
def introduce_errors(text):
    noisy_chars = ['/', '\\', '|', 'l', 'I', '~', '=', '1']
    words = text.split()
    if not words:
        return text

    word_idx = random.randint(0, len(words) - 1)
    word = words[word_idx]

    corruption_type = random.choice([
        'insert_noise', 'break_hyphen', 'merge_hyphenated', 'random_injection'
    ])

    if corruption_type == 'insert_noise':
        if len(word) > 1:
            insert_pos = random.randint(1, len(word) - 1)
            noise = random.choice(noisy_chars)
            word = word[:insert_pos] + noise + word[insert_pos:]

    elif corruption_type == 'break_hyphen' and '-' in word:
        word = word.replace('-', random.choice([' ', '\n', '']))

    elif corruption_type == 'merge_hyphenated' and '-' in word:
        word = word.replace('-', '')

    elif corruption_type == 'random_injection':
        insert_pos = random.randint(0, len(word))
        noise = random.choice(noisy_chars + list('abcdefghijklmnopqrstuvwxyz'))
        word = word[:insert_pos] + noise + word[insert_pos:]

    words[word_idx] = word
    return ' '.join(words)

# Create simulated dirty files from clean files
for clean_file in clean_files:
    clean_file_path = os.path.join(clean_dir, clean_file)

    with open(clean_file_path, 'r', encoding='utf-8') as f:
        clean_text = f.read()

    dirty_text = introduce_errors(clean_text)

    sim_dirty_file_path = os.path.join(sim_dirty_dir, clean_file)
    with open(sim_dirty_file_path, 'w', encoding='utf-8') as f:
        f.write(dirty_text)

# ---------- Load Records ----------
records = []

# Load files from simulated dirty directory
dirty_files = sorted([
    f for f in os.listdir(sim_dirty_dir) if f.endswith(".txt") and os.path.isfile(os.path.join(sim_dirty_dir, f))
])
clean_files = sorted([
    f for f in os.listdir(clean_dir) if f.endswith(".txt") and os.path.isfile(os.path.join(clean_dir, f))
])

if len(dirty_files) != len(clean_files):
    raise ValueError(f"Mismatch in file count: {len(dirty_files)} dirty vs {len(clean_files)} clean")

for dirty_file, clean_file in zip(dirty_files, clean_files):
    with open(os.path.join(sim_dirty_dir, dirty_file), "r", encoding="utf-8") as f_dirty:
        dirty_text = f_dirty.read().strip()
    with open(os.path.join(clean_dir, clean_file), "r", encoding="utf-8") as f_clean:
        clean_text = f_clean.read().strip()
    records.append({"ocr": dirty_text, "clean": clean_text})

print(f"Total records loaded: {len(records)}")

# ---------- Tokenizer & Model ----------
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large")

# ---------- Tokenization ----------
def tokenize(batch):
    input_enc = tokenizer(batch["ocr"], padding="max_length", truncation=True, max_length=1024)
    target_enc = tokenizer(batch["clean"], padding="max_length", truncation=True, max_length=1024)
    input_enc["labels"] = target_enc["input_ids"]
    return input_enc

chunk_size = 10
output_base = "bart_recursive_chunks"

# ---------- Recursive Training ----------
total_chunks = math.ceil(len(records) / chunk_size) # 30/10= 3 chunks. if you have 29, the first chunk will be 10, next is also 10, the third will be 9.
for i in range(total_chunks):
    chunk = records[i * chunk_size : (i + 1) * chunk_size]
    print(f"\n[Chunk {i+1}/{total_chunks}] Training on {len(chunk)} samples...")

    dataset = Dataset.from_list(chunk)
    tokenized_dataset = dataset.map(tokenize, batched=True)

    output_dir = os.path.join(output_base, f"chunk_{i+1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    os.makedirs(output_dir, exist_ok=True)

    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        num_train_epochs=20,
        per_device_train_batch_size=1,
        learning_rate=1e-5, # This value is suitable for Fine-tuning, not training from scratch.
        logging_dir=os.path.join(output_dir, "logs"),
        logging_steps=1,
        save_steps=50,
        eval_strategy="no", # change to 'epoch' in the evaluation code.
        predict_with_generate=True,
        report_to="none"
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer, model)
    )

    trainer.train()
    print(f"[Chunk {i+1}] Done.")

# ---------- Save Final Model ----------
final_path = os.path.join(output_base, "final_model")
model.save_pretrained(final_path)
tokenizer.save_pretrained(final_path)
print(f"\n✅ All chunks trained. Final model saved to: {final_path}")

# Validation
import os
import torch
from datasets import Dataset, load_metric
!pip install rouge_score

# -------- Config --------
validation_dir = "/content/drive/MyDrive/Texts - Validation"  # Folder with .txt files
model_path = "bart_recursive_chunks/final_model"

# -------- Load all txt files --------
records = []
for filename in sorted(os.listdir(validation_dir)):
    if filename.endswith(".txt"):
        with open(os.path.join(validation_dir, filename), "r", encoding="utf-8") as f:
            text = f.read().strip()
            records.append({"ocr": text, "clean": text})  # Same for both

print(f"✅ Loaded {len(records)} validation samples.")

# -------- Load tokenizer & model --------
tokenizer = BartTokenizer.from_pretrained(model_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

# -------- Tokenize --------
def tokenize(batch):
    inputs = tokenizer(batch["ocr"], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(batch["clean"], padding="max_length", truncation=True, max_length=512)
    inputs["labels"] = targets["input_ids"]
    return inputs

dataset = Dataset.from_list(records)
tokenized_val = dataset.map(tokenize, batched=True)

# -------- ROUGE metric --------
rouge = load_metric("rouge")

def compute_metrics(eval_pred):
    preds, labels = eval_pred
    # Filter out None values before decoding
    decoded_preds = [tokenizer.decode(p, skip_special_tokens=True) for p in preds if p is not None]
    decoded_labels = [tokenizer.decode(l, skip_special_tokens=True) for l in labels if l is not None]

    # Ensure lists are not empty before computing metrics
    if not decoded_preds or not decoded_labels:
        return {"rougeL": 0.0}

    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {
        "rougeL": rouge_scores["rougeL"].mid.fmeasure,
    }

# -------- Evaluation setup --------
args = Seq2SeqTrainingArguments(
    output_dir="./eval_tmp",
    per_device_eval_batch_size=1,
    predict_with_generate=True,
    do_train=False,
    do_eval=True,
    logging_dir="./eval_logs",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model),
    compute_metrics=compute_metrics,
)

# -------- Run evaluation --------
print(" Evaluating on txt folder...\n")
metrics = trainer.evaluate()
print(f"\n Results:\nLoss: {metrics['eval_loss']:.4f}\nROUGE-L: {metrics['eval_rougeL']:.4f}")

import json
import torch

# Testing the Fine-tuned model

# Load trained model
model_path = "bart_recursive_chunks/final_model"
tokenizer = BartTokenizer.from_pretrained(model_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

# Load testing data
with open("/content/drive/MyDrive/Vol1StoreOCRPerPage.json", "r", encoding="utf-8") as f:
    ocr_data = json.load(f)

#  Choose one page
page_key = "102"  # Change this to the page you want
# Clean only the selected page
if page_key in ocr_data:
    ocr_text = ocr_data[page_key]
    inputs = tokenizer(ocr_text, return_tensors="pt", truncation=True, padding="max_length", max_length=1024)
    with torch.no_grad(): # test mode
        #output_ids = model.generate(**inputs, max_length=)
        output_ids = model.generate(
    **inputs,
    do_sample=False,          # greedy decoding = less hallucination
    num_beams=1,              # 1 beam = deterministic
    early_stopping=True,      # stop when output is complete
    max_length=1024          # input + output total cap
)
    cleaned_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).replace('-','')

    # Print and save
    print(f"{page_key}:\n{cleaned_text}\n{'='*40}")
    with open("ocr_cleaned_output.json", "w", encoding="utf-8") as f:
        json.dump({page_key: cleaned_text}, f, indent=2, ensure_ascii=False)
    print("\n Cleaned page saved to: ocr_cleaned_output.json")

else:
    print(f" age '{page_key}' not found in ocr_input.json.")

print (ocr_data['583']) # To check the original